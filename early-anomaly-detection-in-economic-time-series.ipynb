{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":1621846,"datasetId":958471,"databundleVersionId":1657540,"isSourceIdPinned":false}],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/angelchaudhary/early-anomaly-detection-in-economic-time-series?scriptVersionId=290380737\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# A Case Study: Early Anomaly Detection in Economic Time Series\n## Identifying unusual spikes before they become crises","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Introduction\nEconomic signals such as inflation rates, stock indices, currency values, or energy prices usually change gradually over time. However, sudden spikes or drops can indicate underlying stress, data issues, or early signs of economic events.\n\nThe challenge is that these anomalies are often not obvious when they first appear. By the time they are visible to humans, the impact may already be significant.\n\nIn this case study, we aim to build a system that can detect unusual behavior in economic time-series data as early as possible, without relying on predefined labels. The focus is not on predicting future values, but on identifying when something abnormal starts happening.\n\n## Approach Structure\n1. Define the anomaly detection objective\n2. Explore and visualize economic time-series signals\n3. Establish naive baselines and their limitations\n4. Frame anomaly detection as a time-aware problem\n5. Engineer features that capture abnormal behavior\n6. Apply multiple anomaly detection techniques\n7. Detect and visualize anomalies over time\n8. Evaluate early-detection performance and stability\n9. Analyze trade-offs, failure cases, and insights\n10. Summarize findings and practical takeaways","metadata":{}},{"cell_type":"markdown","source":"# LET'S DO IT!!!\n![funny gif](https://media.tenor.com/EKmO2VWYor4AAAAM/typing-typing-furiously.gif)","metadata":{}},{"cell_type":"markdown","source":"## 1. Defining the Anomaly Detection Objective\n\nBefore working with data or models, it is important to clearly define what we mean by an *anomaly* in the context of economic signals.\n\nIn this case study, our goal is **not** to predict future values of an economic indicator. Instead, we aim to identify **when the behavior of the signal deviates from its recent historical pattern** in a meaningful way.\n\nKey characteristics of our objective:\n- Anomalies are **contextual**, not absolute  \n- Detection must rely **only on past and present data**  \n- The focus is on **early detection**, not retrospective labeling  \n- The system should work **without predefined anomaly labels**\n \nwe want to flag time points where the current behavior is unlikely under the recent historical distribution.\n\nThis framing allows us to treat anomaly detection as a **time-aware, unsupervised monitoring problem** similar to how real economic surveillance systems operate.\n","metadata":{"execution":{"iopub.status.busy":"2026-01-03T13:45:52.312538Z","iopub.execute_input":"2026-01-03T13:45:52.312921Z","iopub.status.idle":"2026-01-03T13:45:52.424874Z","shell.execute_reply.started":"2026-01-03T13:45:52.312894Z","shell.execute_reply":"2026-01-03T13:45:52.423503Z"}}},{"cell_type":"code","source":"#Global configuration and reproducibility\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Set random seed for reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n#Plotting style\nplt.style.use(\"seaborn-v0_8\")\nplt.rcParams[\"figure.figsize\"] = (12, 5)\n\nprint(\"Environment setup complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:49:01.508515Z","iopub.execute_input":"2026-01-03T13:49:01.508928Z","iopub.status.idle":"2026-01-03T13:49:02.979558Z","shell.execute_reply.started":"2026-01-03T13:49:01.5089Z","shell.execute_reply":"2026-01-03T13:49:02.978319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Exploring and Visualizing the Economic Signal\n\nIn this case study, we use daily historical data from the S&P 500 index as our economic signal.\n\nStock market indices are a useful proxy for overall economic sentiment and systemic stress. They reflect collective expectations, macroeconomic shocks, and periods of instability making them a natural candidate for anomaly detection.\n\nThe dataset spans several decades and includes major economic events such as financial crises, market crashes, and recovery phases. This long historical range allows us to study how “normal” behavior evolves over time and why static anomaly rules often fail.\n\nBefore applying any detection method, we begin by:\n- formatting the time index,\n- selecting a representative signal,\n- and visually inspecting long-term behavior and volatility patterns.\n","metadata":{}},{"cell_type":"code","source":"import kagglehub\n# Download \npath = kagglehub.dataset_download(\"henryhan117/sp-500-historical-data\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:56:06.420558Z","iopub.execute_input":"2026-01-03T13:56:06.42094Z","iopub.status.idle":"2026-01-03T13:56:20.662076Z","shell.execute_reply.started":"2026-01-03T13:56:06.420914Z","shell.execute_reply":"2026-01-03T13:56:20.660865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/sp-500-historical-data/SPX.csv\")\n\ndf.head(), df.info()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:58:27.420355Z","iopub.execute_input":"2026-01-03T13:58:27.42072Z","iopub.status.idle":"2026-01-03T13:58:27.531616Z","shell.execute_reply.started":"2026-01-03T13:58:27.420696Z","shell.execute_reply":"2026-01-03T13:58:27.530438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert Date column to datetime and sort\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf = df.sort_values(\"Date\").reset_index(drop=True)\n\n# Use Adjusted Close as the primary economic signal\ndf = df.rename(columns={\"Adj Close\": \"signal\"})\n\n# Keep only relevant columns for analysis\ndf = df[[\"Date\", \"signal\", \"Volume\"]]\n\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:01:07.883261Z","iopub.execute_input":"2026-01-03T14:01:07.883635Z","iopub.status.idle":"2026-01-03T14:01:08.025103Z","shell.execute_reply.started":"2026-01-03T14:01:07.883609Z","shell.execute_reply":"2026-01-03T14:01:08.02392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic descriptive statistics\ndf[\"signal\"].describe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:01:26.018113Z","iopub.execute_input":"2026-01-03T14:01:26.018516Z","iopub.status.idle":"2026-01-03T14:01:26.039771Z","shell.execute_reply.started":"2026-01-03T14:01:26.018493Z","shell.execute_reply":"2026-01-03T14:01:26.038556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure()\nplt.plot(df[\"Date\"], df[\"signal\"], linewidth=1)\nplt.yscale(\"log\")\nplt.title(\"S&P 500 Index — Adjusted Close (Log Scale)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Index Value (Log Scale)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:02:51.490065Z","iopub.execute_input":"2026-01-03T14:02:51.490427Z","iopub.status.idle":"2026-01-03T14:02:51.978575Z","shell.execute_reply.started":"2026-01-03T14:02:51.490406Z","shell.execute_reply":"2026-01-03T14:02:51.977359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interpreting the Log-Scaled Price Trend\n\nPlotting the S&P 500 index on a logarithmic scale reveals a more stable long-term structure.\n\nOn the log scale, periods of growth and decline become comparable across decades. Major market drawdowns such as crashes and recessions appear as sharp downward deviations from an otherwise smooth upward trajectory.\n\nThis view highlights two important points:\n- the market grows approximately multiplicatively over time rather than additively,\n- large relative drops stand out more clearly than absolute price changes.\n\nWhile log-scaling improves interpretability, it still does not fully separate normal market volatility from genuinely abnormal behavior. Many sharp movements are part of regular market dynamics.\n\nThis reinforces the need to move beyond price levels and focus on **relative changes and volatility-based representations** when detecting anomalies.\n","metadata":{}},{"cell_type":"markdown","source":"## 3. Naive Baselines and Why They Fail\n\nA common first attempt at anomaly detection in time-series data is to assume that extreme deviations from the mean are anomalous.\n\nTypical approaches include:\n- flagging values beyond mean ± k·standard deviations,\n- applying fixed thresholds to returns,\n- or using global statistics computed over the entire dataset.\n\nIn this step, we implement a simple statistical baseline to demonstrate why such methods struggle with economic signals that exhibit non-stationarity and changing volatility.\n\nThe goal is not to build a good detector yet, but to understand **what goes wrong** when naive assumptions are applied.\n","metadata":{}},{"cell_type":"code","source":"# Compute daily log returns (drop first NaN)\n# Create daily log returns\ndf[\"log_return\"] = np.log(df[\"signal\"]).diff()\nreturns = df[\"log_return\"].dropna()\n\nplt.figure()\nplt.plot(df.loc[returns.index, \"Date\"], returns, linewidth=0.8)\nplt.title(\"Daily Log Returns of S&P 500\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Log Return\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:06:15.756285Z","iopub.execute_input":"2026-01-03T14:06:15.75666Z","iopub.status.idle":"2026-01-03T14:06:16.047425Z","shell.execute_reply.started":"2026-01-03T14:06:15.756638Z","shell.execute_reply":"2026-01-03T14:06:16.046059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observing Daily Log Returns\n\nTransforming prices into daily log returns removes the long term growth trend and places different time periods on a comparable scale.\n\nFrom the plot, several important patterns emerge:\n- Volatility is not constant over time, it appears in clusters\n- Early decades show much higher volatility compared to long stable periods\n- Large negative returns occur sporadically and often coincide with known market stress events\n\nWhile extreme return values are now easier to spot, high volatility alone does not necessarily indicate an anomaly. Periods of sustained turbulence are a normal part of financial markets.\n\nThis highlights a key challenge: **anomalies should be defined relative to recent behavior**, not global historical statistics. A method that treats all time periods equally will either overreact during volatile regimes or miss early warning signals in calm ones.","metadata":{}},{"cell_type":"markdown","source":"## 4. Framing Anomaly Detection as a Time-Aware Problem\n\nEconomic systems evolve continuously. What appears abnormal today may become normal tomorrow and vice versa. This makes static anomaly thresholds unreliable.\n\nTo address this, anomaly detection must be **time-aware**:\n- statistics should be computed using only recent history\n- thresholds should adapt as volatility changes\n- and future data must never influence past decisions.\n\nIn this step, we introduce a rolling-window baseline that continuously updates its understanding of “normal” behavior and flags deviations relative to the current regime.\n","metadata":{}},{"cell_type":"code","source":"# Rolling window size (in trading days)\nWINDOW_SIZE = 252  # ~1 trading year\n\ndf[\"rolling_mean\"] = df[\"log_return\"].rolling(WINDOW_SIZE).mean()\ndf[\"rolling_std\"] = df[\"log_return\"].rolling(WINDOW_SIZE).std()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:09:34.684502Z","iopub.execute_input":"2026-01-03T14:09:34.684889Z","iopub.status.idle":"2026-01-03T14:09:34.699337Z","shell.execute_reply.started":"2026-01-03T14:09:34.684867Z","shell.execute_reply":"2026-01-03T14:09:34.698327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"K = 3  # sensitivity parameter\n\ndf[\"upper_bound\"] = df[\"rolling_mean\"] + K * df[\"rolling_std\"]\ndf[\"lower_bound\"] = df[\"rolling_mean\"] - K * df[\"rolling_std\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:09:43.627626Z","iopub.execute_input":"2026-01-03T14:09:43.628917Z","iopub.status.idle":"2026-01-03T14:09:43.637092Z","shell.execute_reply.started":"2026-01-03T14:09:43.628881Z","shell.execute_reply":"2026-01-03T14:09:43.636147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"rolling_anomaly\"] = (\n    (df[\"log_return\"] > df[\"upper_bound\"]) |\n    (df[\"log_return\"] < df[\"lower_bound\"])\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:09:51.191951Z","iopub.execute_input":"2026-01-03T14:09:51.1923Z","iopub.status.idle":"2026-01-03T14:09:51.201357Z","shell.execute_reply.started":"2026-01-03T14:09:51.192274Z","shell.execute_reply":"2026-01-03T14:09:51.200195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure()\nplt.plot(df[\"Date\"], df[\"log_return\"], label=\"Log Returns\", alpha=0.6)\nplt.plot(df[\"Date\"], df[\"upper_bound\"], linestyle=\"--\", color=\"gray\", alpha=0.8)\nplt.plot(df[\"Date\"], df[\"lower_bound\"], linestyle=\"--\", color=\"gray\", alpha=0.8)\n\nplt.scatter(\n    df.loc[df[\"rolling_anomaly\"], \"Date\"],\n    df.loc[df[\"rolling_anomaly\"], \"log_return\"],\n    color=\"red\",\n    s=10,\n    label=\"Detected Anomalies\"\n)\n\nplt.title(\"Time-Aware Anomaly Detection Using Rolling Statistics\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Log Return\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:10:10.306744Z","iopub.execute_input":"2026-01-03T14:10:10.307108Z","iopub.status.idle":"2026-01-03T14:10:10.742517Z","shell.execute_reply.started":"2026-01-03T14:10:10.307088Z","shell.execute_reply":"2026-01-03T14:10:10.741154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observations from Rolling-Window Anomaly Detection\n\nUsing rolling statistics allows the anomaly detection threshold to adapt to local market conditions rather than relying on a single global baseline.\n\nFrom the visualization, we observe that:\n- anomaly thresholds widen during high volatility regimes and contract during calm periods,\n- extreme return events are detected relative to their immediate historical context,\n- false positives are reduced compared to global thresholding.\n\nHowever, a key limitation remains. Many detected anomalies occur **after volatility has already increased**, rather than at the very beginning of a regime change. This means the method is reactive rather than predictive.\n\nWhile rolling window statistics improve robustness, they are still insufficient as an early warning mechanism on their own. Detecting changes in volatility itself may provide earlier signals of abnormal behavior.\n","metadata":{}},{"cell_type":"markdown","source":"## 5. Detecting Volatility Regime Shifts\n\nSo far, anomaly detection has focused on extreme return values. However, in many economic systems, risk does not emerge suddenly it builds up gradually through increasing uncertainty.\n\nVolatility captures this uncertainty. Sudden increases in volatility often precede major market disruptions and signal regime changes rather than isolated shocks. In this step, we treat **volatility itself as the signal** and aim to detect abnormal increases in volatility as early warning indicators.","metadata":{}},{"cell_type":"code","source":"# Rolling volatility (standard deviation of returns)\nVOL_WINDOW = 63  # ~3 months\n\ndf[\"rolling_volatility\"] = df[\"log_return\"].rolling(VOL_WINDOW).std()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:13:06.334576Z","iopub.execute_input":"2026-01-03T14:13:06.334956Z","iopub.status.idle":"2026-01-03T14:13:06.342934Z","shell.execute_reply.started":"2026-01-03T14:13:06.334936Z","shell.execute_reply":"2026-01-03T14:13:06.341847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure()\nplt.plot(df[\"Date\"], df[\"rolling_volatility\"], linewidth=1)\nplt.title(\"Rolling Volatility of S&P 500 Returns\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Volatility\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:13:12.255671Z","iopub.execute_input":"2026-01-03T14:13:12.256773Z","iopub.status.idle":"2026-01-03T14:13:12.502878Z","shell.execute_reply.started":"2026-01-03T14:13:12.25674Z","shell.execute_reply":"2026-01-03T14:13:12.501581Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observing Rolling Volatility\n\nThe rolling volatility plot reveals clear regime dependent behavior in the S&P 500.\n\nKey observations:\n- Volatility is not constant and tends to cluster over time.\n- Extended periods of low volatility are punctuated by sharp spikes during market stress.\n- Early decades exhibit structurally higher volatility compared to later periods, reflecting different market conditions and liquidity.\n\nThese patterns suggest that volatility itself carries meaningful information about the underlying state of the system. Rising volatility often signals increasing uncertainty before large price movements occur, making it a valuable signal for early anomaly detection.\n","metadata":{}},{"cell_type":"code","source":"# Rolling baseline for volatility\nVOL_BASELINE_WINDOW = 252  # ~1 year\n\ndf[\"vol_mean\"] = df[\"rolling_volatility\"].rolling(VOL_BASELINE_WINDOW).mean()\ndf[\"vol_std\"] = df[\"rolling_volatility\"].rolling(VOL_BASELINE_WINDOW).std()\n\nVOL_K = 2.5\n\ndf[\"vol_upper_bound\"] = df[\"vol_mean\"] + VOL_K * df[\"vol_std\"]\n\ndf[\"vol_anomaly\"] = df[\"rolling_volatility\"] > df[\"vol_upper_bound\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:13:38.757031Z","iopub.execute_input":"2026-01-03T14:13:38.758071Z","iopub.status.idle":"2026-01-03T14:13:38.771503Z","shell.execute_reply.started":"2026-01-03T14:13:38.758026Z","shell.execute_reply":"2026-01-03T14:13:38.770019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure()\nplt.plot(df[\"Date\"], df[\"rolling_volatility\"], label=\"Rolling Volatility\", alpha=0.8)\nplt.plot(df[\"Date\"], df[\"vol_upper_bound\"], linestyle=\"--\", color=\"gray\", label=\"Adaptive Threshold\")\n\nplt.scatter(\n    df.loc[df[\"vol_anomaly\"], \"Date\"],\n    df.loc[df[\"vol_anomaly\"], \"rolling_volatility\"],\n    color=\"red\",\n    s=15,\n    label=\"Volatility Anomalies\"\n)\n\nplt.title(\"Volatility-Based Anomaly Detection\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Volatility\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:13:46.980743Z","iopub.execute_input":"2026-01-03T14:13:46.981359Z","iopub.status.idle":"2026-01-03T14:13:47.561736Z","shell.execute_reply.started":"2026-01-03T14:13:46.981321Z","shell.execute_reply":"2026-01-03T14:13:47.560204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interpreting Volatility-Based Anomaly Detection\n\nApplying an adaptive threshold to rolling volatility allows anomalies to be detected relative to the recent volatility regime.\n\nFrom the plot, we observe that:\n- volatility anomalies align closely with known periods of market stress,\n- detection occurs during sustained increases in uncertainty rather than isolated spikes,\n- false positives are reduced during stable periods compared to return-based detection.\n\nHowever, volatility anomalies tend to persist once triggered, indicating prolonged risk regimes rather than single anomalous events. This reinforces the interpretation of volatility anomalies as indicators of **risk escalation**, not precise timing signals.\n","metadata":{}},{"cell_type":"markdown","source":"## 6. Combining Return and Volatility Signals\n\nSo far, we have explored two complementary signals:\n- extreme returns, which capture sudden shocks,\n- abnormal volatility, which captures rising uncertainty and regime changes.\n\nIndividually, each signal has limitations. Return-based detection is often reactive, while volatility-based detection can remain elevated for long periods.\n\nIn practice, anomaly detection systems combine multiple weak signals to produce a more reliable indicator. In this step, we construct a simple composite anomaly score that incorporates both return deviations and volatility shifts.\n","metadata":{}},{"cell_type":"code","source":"# Standardized return deviation\ndf[\"return_zscore\"] = (\n    (df[\"log_return\"] - df[\"rolling_mean\"]) / df[\"rolling_std\"]\n)\n\n# Standardized volatility deviation\ndf[\"vol_zscore\"] = (\n    (df[\"rolling_volatility\"] - df[\"vol_mean\"]) / df[\"vol_std\"]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:15:43.840816Z","iopub.execute_input":"2026-01-03T14:15:43.841203Z","iopub.status.idle":"2026-01-03T14:15:43.857634Z","shell.execute_reply.started":"2026-01-03T14:15:43.841171Z","shell.execute_reply":"2026-01-03T14:15:43.856169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Composite anomaly score\ndf[\"anomaly_score\"] = (\n    df[\"return_zscore\"].abs() +\n    df[\"vol_zscore\"].clip(lower=0)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:15:55.333297Z","iopub.execute_input":"2026-01-03T14:15:55.333602Z","iopub.status.idle":"2026-01-03T14:15:55.343755Z","shell.execute_reply.started":"2026-01-03T14:15:55.333581Z","shell.execute_reply":"2026-01-03T14:15:55.342528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure()\nplt.plot(df[\"Date\"], df[\"anomaly_score\"], linewidth=1)\nplt.title(\"Composite Anomaly Score (Returns + Volatility)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Anomaly Score\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:16:02.979588Z","iopub.execute_input":"2026-01-03T14:16:02.979934Z","iopub.status.idle":"2026-01-03T14:16:03.339462Z","shell.execute_reply.started":"2026-01-03T14:16:02.979908Z","shell.execute_reply":"2026-01-03T14:16:03.33839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interpreting the Composite Anomaly Score\n\nThe composite anomaly score aggregates information from both return deviations and volatility shifts into a single time-varying measure of abnormal behavior.\n\nFrom the plot, we observe that:\n- the score remains low and stable during normal market conditions,\n- sharp spikes occur during periods of elevated market stress,\n- extreme peaks correspond to major systemic disruptions rather than isolated price movements.\n\nUnlike return-based signals alone, the composite score rises during sustained periods of uncertainty, not just on individual shock days. This makes it suitable for monitoring broader risk regimes instead of single-event detection.\n","metadata":{}},{"cell_type":"code","source":"# Threshold based on high percentile\nSCORE_THRESHOLD = df[\"anomaly_score\"].quantile(0.99)\n\ndf[\"composite_anomaly\"] = df[\"anomaly_score\"] > SCORE_THRESHOLD\n\nprint(f\"Composite anomalies detected: {df['composite_anomaly'].sum()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:16:21.337131Z","iopub.execute_input":"2026-01-03T14:16:21.337488Z","iopub.status.idle":"2026-01-03T14:16:21.348835Z","shell.execute_reply.started":"2026-01-03T14:16:21.337466Z","shell.execute_reply":"2026-01-03T14:16:21.347681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure()\nplt.plot(df[\"Date\"], df[\"log_return\"], alpha=0.6, label=\"Log Returns\")\n\nplt.scatter(\n    df.loc[df[\"composite_anomaly\"], \"Date\"],\n    df.loc[df[\"composite_anomaly\"], \"log_return\"],\n    color=\"red\",\n    s=12,\n    label=\"Composite Anomalies\"\n)\n\nplt.title(\"Composite Anomaly Detection on Returns\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Log Return\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:16:32.646306Z","iopub.execute_input":"2026-01-03T14:16:32.646689Z","iopub.status.idle":"2026-01-03T14:16:32.998546Z","shell.execute_reply.started":"2026-01-03T14:16:32.646666Z","shell.execute_reply":"2026-01-03T14:16:32.997473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Composite Anomalies on Returns\n\nOverlaying composite anomalies on the return series highlights how the detector behaves at the event level.\n\nKey observations include:\n- anomalies cluster around periods of heightened volatility rather than appearing randomly,\n- both positive and negative extreme returns are captured,\n- isolated return spikes during otherwise calm periods are less frequently flagged.\n\nThis confirms that the composite detector prioritizes context over magnitude. An extreme return is only flagged when it coincides with abnormal volatility behavior, reducing false alarms while still capturing meaningful market stress events.\n","metadata":{}},{"cell_type":"markdown","source":"## 7. Evaluating Early Detection Behavior\n\nAn effective anomaly detection system is not judged only by how extreme its detections look but by **when** it reacts.\n\nIn economic monitoring, early warning is more valuable than precise event timing. The key questions we aim to answer are:\n- Does the anomaly signal rise before major disruptions?\n- Is the signal stable, or does it fluctuate excessively?\n- Does it highlight sustained risk periods rather than isolated noise?\n\nIn this step, we evaluate the temporal behavior of the composite anomaly score to understand its usefulness as an early warning indicator.\n","metadata":{}},{"cell_type":"code","source":"# Smooth anomaly score to observe buildup\nSMOOTH_WINDOW = 21  # ~1 trading month\n\ndf[\"smoothed_score\"] = df[\"anomaly_score\"].rolling(SMOOTH_WINDOW).mean()\n\nplt.figure()\nplt.plot(df[\"Date\"], df[\"smoothed_score\"], linewidth=1)\nplt.title(\"Smoothed Composite Anomaly Score\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Smoothed Anomaly Score\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:18:34.152239Z","iopub.execute_input":"2026-01-03T14:18:34.153517Z","iopub.status.idle":"2026-01-03T14:18:34.461062Z","shell.execute_reply.started":"2026-01-03T14:18:34.153483Z","shell.execute_reply":"2026-01-03T14:18:34.459722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observing the Smoothed Composite Anomaly Score\n\nSmoothing the composite anomaly score reveals its underlying trend rather than reacting to day-to-day noise.\n\nFrom the plot, we observe that:\n- elevated anomaly scores often persist over extended periods,\n- peaks emerge gradually rather than appearing only on single extreme days,\n- major spikes correspond to prolonged phases of market instability.\n\nThis behavior indicates that the detector captures **risk accumulation**, not just isolated shocks. The gradual rise in the smoothed score before extreme events suggests that the system provides early signals of abnormal market conditions, which is more valuable for monitoring than pinpoint event prediction.\n","metadata":{}},{"cell_type":"code","source":"# Define extreme return days\nEXTREME_RETURN_THRESHOLD = returns.abs().quantile(0.995)\n\ndf[\"extreme_return\"] = df[\"log_return\"].abs() > EXTREME_RETURN_THRESHOLD\n\n# Fraction of extreme return days preceded by elevated anomaly score\nPRE_WINDOW = 10  # days before event\n\nearly_hits = []\n\nfor idx in df.index[df[\"extreme_return\"]]:\n    start = max(idx - PRE_WINDOW, 0)\n    if df.loc[start:idx, \"anomaly_score\"].max() > SCORE_THRESHOLD:\n        early_hits.append(True)\n    else:\n        early_hits.append(False)\n\nearly_hit_rate = np.mean(early_hits)\n\nearly_hit_rate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:19:00.332055Z","iopub.execute_input":"2026-01-03T14:19:00.332671Z","iopub.status.idle":"2026-01-03T14:19:00.35864Z","shell.execute_reply.started":"2026-01-03T14:19:00.332641Z","shell.execute_reply":"2026-01-03T14:19:00.357606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Early Detection Results\n\nUsing a 10-day lookback window, approximately **45% of extreme return events** were preceded by elevated composite anomaly scores.\n\nThis indicates that nearly half of the most severe market movements were not sudden surprises, but were instead preceded by detectable increases in abnormal behavior such as rising volatility or unstable returns.\n\nWhile this does not imply precise prediction of crash days, it demonstrates meaningful early warning capability in a realistic, unsupervised setting.\n","metadata":{}},{"cell_type":"markdown","source":"## Key Insights & Failure Modes\n\nThis case study highlights several important insights about anomaly detection in economic time series.\n\nFirst, anomalies in economic signals are **contextual**, not absolute. What looks extreme in one period may be normal in another. This makes fixed thresholds unreliable.\n\nSecond, return-based anomaly detection is useful for identifying shocks, but it is often **reactive**. By the time extreme returns appear, market stress has usually already escalated.\n\nThird, volatility-based signals provide **earlier warnings**. Rising volatility often precedes major disruptions, making it a valuable indicator of increasing risk.\n\nHowever, the approach also has limitations:\n- volatility anomalies can persist for long periods, reducing timing precision\n- not all extreme events are preceded by clear warning signals\n- unsupervised methods cannot distinguish between harmful and benign volatility without external context\n\nThese limitations are inherent to real-world economic monitoring and highlight the trade off between early warning and false alarms.\n","metadata":{}},{"cell_type":"markdown","source":"## Practical Takeaways\n\nFrom a practical perspective, this study suggests that anomaly detection systems should be designed as **monitoring tools**, not prediction engines.\n\nKey takeaways include:\n- anomaly scores are best used to flag periods of elevated risk rather than exact event dates,\n- combining multiple signals leads to more robust detection than relying on a single indicator,\n- early warning signals should trigger closer inspection, not automatic decisions.\n\nIn real applications, such systems are most effective when paired with human judgment, domain knowledge, and additional contextual information.\n","metadata":{}},{"cell_type":"markdown","source":"## Conclusion & Future Work\n\nIn this notebook, we explored anomaly detection in economic time-series data using the S&P 500 as a real-world signal.\n\nWe showed that:\n- naive global thresholds fail in non-stationary environments,\n- time-aware rolling methods improve robustness,\n- volatility-based signals provide earlier warnings,\n- combining returns and volatility yields a more stable and interpretable anomaly score.\n\nWhile the proposed approach is simple and fully interpretable, it can be extended in several directions. Future work could include:\n- multivariate signals across multiple economic indicators,\n- regime-switching or state-space models,\n- online learning methods that adapt more quickly to structural change,\n- evaluation using known historical events as weak labels.\n\nOverall, this case study demonstrates how anomaly detection can be used to build **early warning systems** for economic monitoring, focusing on stability, interpretability, and real-world reliability rather than perfect prediction.\n","metadata":{}}]}